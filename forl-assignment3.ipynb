{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a83e72",
   "metadata": {},
   "source": [
    "# Exercise 1. Understanding offline and online imitation learning\n",
    "\n",
    "In this problem, you will focus on imitation learning (IL) and get more insights for behavioral cloning (BC), data aggregation with DAgger, and policy aggregation via SMile. You will play with a classical robotic control task in [OpenAI Gym](https://gym.openai.com/), i.e., the [CartPole](https://gym.openai.com/envs/CartPole-v1/) environment.\n",
    "\n",
    "\n",
    "This problem focuses on a regime with a low data rate (small training set) so that it is straightforward to compare the efficiency of offline and online IL approaches. Efficiency here refers to i) the number of demonstrations needed from the expert; and ii) the number of updates to learn the policy. This assignment can run perfectly on a laptop.\n",
    "\n",
    "**Randomness and reproducibility.**\n",
    "We employ pseudo-randomness to ensure randomness and reproducibility at the same time, as was done in most of deep learning tasks. To this end, there are two lines of code that you will see a few times.\n",
    "\n",
    "- `seed_everything(seed)`\n",
    "- `env.reset(seed=seed)`\n",
    "\n",
    "The first line resets the seed for packages such as `torch`, and `numpy`. This will be used to ensure that the neural network parameters are initialized the same (if necessary). The second is used to cope with the randomness for `CartPole` environment. More details will be provided on the latter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf241006",
   "metadata": {},
   "source": [
    "###  Step 1. Set up environments\n",
    "\n",
    "The first step is to get familiar with packages. We will work with libraries `torch`, `gym`, and `stable_baselines3`. Most people should be familiar with `torch` already, hence we put more emphasis on the other two packages.\n",
    "\n",
    "- [`gym`](https://www.gymlibrary.dev/index.html) and [`gymnasium`](https://gymnasium.farama.org/) provide APIs for simulating a control problem (e.g., CartPole), a game (e.g., Atari), and other environments for RL.\n",
    "\n",
    "\n",
    "- [`stable_baselines3`](https://stable-baselines3.readthedocs.io/en/master/) implements various RL algorithms. These algorithms can be easily applied on environments from `gym`. \n",
    "\n",
    "\n",
    "Typically, you can use `pip` or `conda` to install these packages. Just in case that there are problems with package versions, we have tested our code in the environment below `python 3.9.7`, `torch 2.2.2`, `gym 0.26.2`, `gymnasium 0.29.1`. It is recommended to install packages yourself, but if it does not work, you can run the command below for installing needed packages. \n",
    "```bash\n",
    "conda env create -n forl -f enviroment.yml\n",
    "conda activate forl\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "You will focus on the [**CarPole**](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) in this problem. In particular, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. The goal is to keep the pole upright for as long as possible.\n",
    "A graphical illustration can be found [here](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "\n",
    "In the framework of RL, this problem can be defined by\n",
    "\n",
    "- **State.** The observation is a ndarray with shape (4,) with the values corresponding to\n",
    "\n",
    "| state    | meaning               | min            | max\n",
    "| -------- | -------               | -----          | -----\n",
    "| state(0) | cart position         | - 4.8          | 4.8\n",
    "| state(1) | cart velocity         | - inf          | inf\n",
    "| state(2) | pole angle            | -24$^\\circ$    | 24$^\\circ$ \n",
    "| state(3) | pole angular velocity | - inf          | inf\n",
    "\n",
    "\n",
    "- **Action.** An action is a ndarray with shape (1,) which can take values {0, 1} to indicate the direction of the fixed force the cart is pushed with. 0 and 1 mean to push the cart to left and right, respectively.\n",
    "\n",
    "- **Reward.** Since the goal is to keep the pole upright for as long as possible, a reward is +1 for every step taken, including the termination step.\n",
    "\n",
    "- **Initialization.** A uniformly random value in (-0.05, 0.05)\n",
    "\n",
    "- **Episode end.** The episode ends if any one of the following occurs: 1) pole angle is greater than $\\pm 12^\\circ$; 2) cart position is greater than $\\pm 2.4$ (center of the cart reaches the edge of the display); or 3) episode length is greater than 500.\n",
    "\n",
    "The last termination condition means that the largest reward to achieve is $500$.\n",
    "\n",
    "\n",
    "<b style=\"color:red\">Question 1 (5 pts).</b> Install necessary packages, such that the RL environment can be set up by running the code below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f526b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f7e6c",
   "metadata": {},
   "source": [
    "### Step 2. Preparation\n",
    "\n",
    "Once the environment is set up, you can prepare the expert needed for IL. You do not need to code anything until this point, although you can play with the hyperparameters for fun.\n",
    "\n",
    "\n",
    "The expert is a multi-layer neural network trained by PPO. Training takes a few minutes (typically less than 5 min).\n",
    "Running the code below, you can see that the reward is 500 after training. This means that your expert can solve the CartPole *perfectly*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ae434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"\n",
    "    This is used for reproducing. Do not modify.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "#     sp.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "\n",
    "seed_everything(12345)\n",
    "\n",
    "\n",
    "# Initialize our expert\n",
    "# Note: you can also set verbose=1 to see the training logs.\n",
    "expert = PPO('MlpPolicy', env_name, learning_rate=2e-4, verbose=0, seed=111)\n",
    "\n",
    "# Check the architecture of policy neural network\n",
    "print('========= Policy Net =========')\n",
    "print(expert.policy)\n",
    "print('========= Policy Net =========')\n",
    "\n",
    "# Evaluate your expert before training\n",
    "env.reset(seed=10)\n",
    "mean_reward, std_reward = evaluate_policy(expert, env, n_eval_episodes=10)\n",
    "print(f'Before training, the reward of our expert is {mean_reward} +/- {std_reward}')\n",
    "\n",
    "print('========= Training ==========')\n",
    "expert.learn(total_timesteps=5e4) \n",
    "\n",
    "# Evaluate your expert after training\n",
    "env.reset(seed=10)\n",
    "mean_reward, std_reward = evaluate_policy(expert, env, n_eval_episodes=10)\n",
    "print(f'After training, the reward of our expert is {mean_reward} +/- {std_reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5eef77",
   "metadata": {},
   "source": [
    "### Step 3. Offline IL with Behavioral Cloning (BC) <b style=\"color:red\">(5 points) </b>\n",
    "\n",
    "You will work with an offline IL algorithm -- BC. Read the code below, you will be asked to perform some analysis. There is no need to tune hyperparameters, but you can play with them for better understanding.\n",
    "\n",
    "**When hand in this assignment, please use hyperparameters provided.**\n",
    "\n",
    "\n",
    "**Note on randomness**. Our protocol is that we keep a seed in IL algorithms (e.g., BC). Each time after calling `env.reset(seed=seed)`, we increment seed by 1 to ensure randomness and reproducibility. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExpertData(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for expert demonstrations\n",
    "\n",
    "    :param expert_observations: tensor of size [num_demonstrations, dim_state] \n",
    "    :param expert_actions: tensor of size [num_demonstrations, dim_action] \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    \n",
    "class BehavioralCloning:\n",
    "    def __init__(self, env, expert, student, args):\n",
    "        \"\"\"\n",
    "        Main class for BC\n",
    "\n",
    "        :param env: the CartPole environment\n",
    "        :param expert: the expert to learn from (which is trained with PPO previously). In BC, it only provides demonstrations.\n",
    "        :param student: the policy we hope to learn from expert demonstrations\n",
    "        :param args: hyperparameters for training, you do not need to tune them. \n",
    "                     when hand in this assignment, please use the args provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.student = student\n",
    "        self.expert = expert\n",
    "        self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 0\n",
    "        \n",
    "        \n",
    "    def expert_demonstration(self, num_data=200):\n",
    "        \"\"\"\n",
    "        Prepare expert demonstrations\n",
    "        \n",
    "        :param num_data: number of demonstrations\n",
    "        :return ExpertData: the dataset contains expert demonstrations\n",
    "        \"\"\"\n",
    "        \n",
    "        observations = np.empty((num_data,) + self.env.observation_space.shape)\n",
    "        actions = np.empty((num_data,) + self.env.action_space.shape)\n",
    "        \n",
    "        obs, _ = self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "\n",
    "        for i in range(int(num_data)):\n",
    "            action, _ = self.expert.predict(obs, deterministic=True)\n",
    "            observations[i], actions[i] = obs, action\n",
    "            obs, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "            if done:\n",
    "                obs, _ = self.env.reset(seed=self.seed)\n",
    "                self.seed += 1\n",
    "            \n",
    "        return ExpertData(observations, actions)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Using BC to learn a policy out of expert demonstrations\n",
    "        \"\"\"\n",
    "        \n",
    "        # prepare expert demonstrations\n",
    "        demonstration = self.expert_demonstration(num_data=self.args['num_expert_demo'])\n",
    "        demonstration = torch.utils.data.DataLoader(dataset=demonstration, batch_size=self.args['bs'], shuffle=True)\n",
    "        \n",
    "        # prepare student, optimizer\n",
    "        model = self.student.policy.to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.args['lr'])\n",
    "        lr_scheduler = StepLR(optimizer, step_size=1, gamma=self.args['schedular_gamma'])\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(1, self.args['epochs'] + 1):\n",
    "            for batch_idx, (data, target) in enumerate(demonstration):\n",
    "                \n",
    "                # get expert demonstration\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # get student response\n",
    "                dist = model.get_distribution(data)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target = target.long()\n",
    "                \n",
    "                # update student\n",
    "                loss = criterion(action_prediction, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            lr_scheduler.step()\n",
    "                \n",
    "        self.student.policy = model\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        evaluate a policy\n",
    "        \"\"\"\n",
    "        self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "        self.student.policy.eval()\n",
    "        mean_reward, std_reward = evaluate_policy(self.student, self.env, n_eval_episodes=10)\n",
    "        return mean_reward, std_reward\n",
    "        \n",
    "\n",
    "# training arguments for BC\n",
    "bc_args = dict(\n",
    "    lr = 0.01,                  # learning rate\n",
    "    bs = 2,                     # batch size\n",
    "    schedular_gamma = 0.7,      # learning rate scheduler\n",
    "    epochs = 5,                 # number of training epochs\n",
    "    num_expert_demo = 400,      # number of expert demonstrations\n",
    ")        \n",
    "\n",
    "seed_everything(30)\n",
    "\n",
    "# The student that learns from expert's demonstration \n",
    "# For this question, it will be a MLP trained with A2C\n",
    "student = A2C('MlpPolicy', env_name, verbose=1)\n",
    "bc = BehavioralCloning(env, expert, student, bc_args)\n",
    "\n",
    "mean, std = bc.evaluate()\n",
    "print('=' * 20)\n",
    "print(f'Before applying BC, the reward of student is {mean} +/- {std}')\n",
    "\n",
    "bc.train()\n",
    "mean, std = bc.evaluate()\n",
    "print('=' * 20)\n",
    "print(f'After applying BC, the reward of student is {mean} +/- {std}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792c313",
   "metadata": {},
   "source": [
    "**Training results.** In the small data regime, we train BC for 5 epochs to ensure sufficient convergence. The reward of BC should be around 450, which is clearly not optimal. Next you need to perform some analyses on the data efficiency.\n",
    "\n",
    "<b style=\"color:red\">Question 2 (5 pts).</b> In the code above, how many demonstrations are needed from experts? How many optimization steps are taken?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdcce1c",
   "metadata": {},
   "source": [
    "400 expert demonstrations.\n",
    "The total number of optimization steps is equal to $\\frac{\\text{total number of demonstrations}}{\\text{batch size}} \\times \\text{epochs}$. Therefore we have a total number of 1000 optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af9e05",
   "metadata": {},
   "source": [
    "### Step 3. IL with data aggregation  <b style=\"color:red\">(20 points) </b>\n",
    "\n",
    "Next, you will focus on DAgger, and compare its efficiency with BC. The DAgger can be summarized as\n",
    "\n",
    "- Step 1. Train $\\pi$ from expert's demonstrations ${\\cal D}= \\{ s_1, a_1, s_2, a_2, ...\\}$\n",
    "\n",
    "- Step 2. Run $\\pi$ to get states $\\hat{\\cal D}_\\pi = \\{ s_1, s_2, .... \\}$, and label these states using expert demonstrations ${\\cal D}_\\pi = \\{ s_1, a_1, s_2, a_2 .... \\}$\n",
    "\n",
    "- Step 3. Aggregate ${\\cal D} \\leftarrow {\\cal D} \\cup {\\cal D}_\\pi$ and go to step 1\n",
    "\n",
    "<b style=\"color:red\">Question 3 (10 pts).</b> Implement and test DAgger by completing lines marked with **#TODO**. After the code is finished, it should get a final reward of 500. There is no need to tune hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ed584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAgger:\n",
    "    def __init__(self, env, expert, student, args):\n",
    "        \"\"\"\n",
    "        Main class for DAgger. \n",
    "\n",
    "        :param env: the CartPole environment\n",
    "        :param expert: the expert to learn from (which is trained with PPO previously).\n",
    "        :param student: the policy we hope to learn from expert demonstrations\n",
    "        :param args: hyperparameters for training, you do not need to tune them. \n",
    "                     when hand in this assignment, please use the args provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.student = student\n",
    "        self.expert = expert\n",
    "        self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 0\n",
    "        \n",
    "    def expert_demonstration(self, num_data=200):\n",
    "        \"\"\"\n",
    "        Prepare expert demonstrations. \n",
    "        This is the same as BehavioralCloning.expert_demonstration()\n",
    "        \"\"\"\n",
    "        observations = np.empty((num_data,) + self.env.observation_space.shape)\n",
    "        actions = np.empty((num_data,) + self.env.action_space.shape)\n",
    "        obs, _ = self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "\n",
    "        for i in range(int(num_data)):\n",
    "            action, _ = self.expert.predict(obs, deterministic=True)\n",
    "            observations[i], actions[i] = obs, action\n",
    "            obs, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "            if done:\n",
    "                obs, _ = self.env.reset(seed=self.seed)\n",
    "                self.seed += 1\n",
    "            \n",
    "        return ExpertData(observations, actions)\n",
    "    \n",
    "\n",
    "    def student_transition_with_expert_demonstration(self, num_data=200):\n",
    "        \"\"\"\n",
    "        Implementation of step 2 and step 3.\n",
    "        \n",
    "        Note: please follow our reproducibility protocol when using 'env.reset(seed=self.seed)'\n",
    "        \n",
    "        :param num_data: number of demonstrations\n",
    "        :return ExpertData: the dataset containing expert demonstrations\n",
    "        \"\"\"\n",
    "        \n",
    "        observations = np.empty((num_data,) + self.env.observation_space.shape)\n",
    "        actions = np.empty((num_data,) + self.env.action_space.shape)\n",
    "        \n",
    "        obs, _ = self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "        \n",
    "        # TODO: get expert demonstrations\n",
    "        # update: there is a loop, and the if-done code below is inside this loop\n",
    "\n",
    "        for i in range(int(num_data)):\n",
    "            expert_action, _ = self.expert.predict(obs, deterministic=True)\n",
    "            student_action, _ = self.student.predict(obs, deterministic=True)\n",
    "            # append the observation and expert action to the dataset\n",
    "            observations[i], actions[i] = obs, expert_action\n",
    "            # get the next observation, using the student_action\n",
    "            obs, reward, done, _, _ = self.env.step(student_action)\n",
    "        \n",
    "            if done:\n",
    "                obs, _ = self.env.reset(seed=self.seed)\n",
    "                self.seed += 1\n",
    "        \n",
    "            \n",
    "        return ExpertData(observations, actions)\n",
    "    \n",
    "    \n",
    "    def bc_train(self, demonstration: ExpertData):   \n",
    "        \"\"\"\n",
    "        Train the student using behavioral cloning. The code here is modified from BehavioralCloning.train()\n",
    "        \n",
    "        :param demonstration: ExpertData that contains expert demonstration\n",
    "        \"\"\"\n",
    "        \n",
    "        # prepare student, optimizer\n",
    "        model = self.student.policy.to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.args['lr'])\n",
    "        lr_scheduler = StepLR(optimizer, step_size=1, gamma=self.args['schedular_gamma'])\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(1, self.args['epochs'] + 1):\n",
    "            for batch_idx, (data, target) in enumerate(demonstration):\n",
    "                \n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # get student response\n",
    "                dist = model.get_distribution(data)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target = target.long()\n",
    "                \n",
    "                # update student\n",
    "                loss = criterion(action_prediction, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            lr_scheduler.step()\n",
    "                \n",
    "        self.student.policy = model\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        train a policy out of expert demonstrations using DAgger\n",
    "        \"\"\"\n",
    "\n",
    "        demonstration_dataset = self.expert_demonstration(num_data=self.args['num_expert_demo'])\n",
    "        demonstration = torch.utils.data.DataLoader(dataset=demonstration_dataset, batch_size=self.args['bs'], shuffle=True)\n",
    "        \n",
    "        # prepare student, optimizer\n",
    "        model = self.student.policy.to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.args['lr'])\n",
    "        lr_scheduler = StepLR(optimizer, step_size=1, gamma=self.args['schedular_gamma'])\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for dagger_round in range(self.args['dagger_rounds']):\n",
    "            # TODO: Training using DAgger\n",
    "            for epoch in range(1, self.args['epochs'] + 1):\n",
    "                for batch_idx, (data, target) in enumerate(demonstration):\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # get student response\n",
    "                    dist = model.get_distribution(data)\n",
    "                    action_prediction = dist.distribution.logits\n",
    "                    target = target.long()\n",
    "                    \n",
    "                    # update student\n",
    "                    loss = criterion(action_prediction, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                lr_scheduler.step()\n",
    "                # aggregate data\n",
    "                demonstration_student_trans = self.student_transition_with_expert_demonstration(num_data=self.args['num_expert_demo_dagger'][dagger_round])\n",
    "                demonstration_dataset = torch.utils.data.ConcatDataset([demonstration_dataset, demonstration_student_trans])\n",
    "                print(len(demonstration_dataset))\n",
    "                demonstration = torch.utils.data.DataLoader(dataset=demonstration_dataset, batch_size=self.args['bs'], shuffle=True)\n",
    "        \n",
    "\n",
    "            \n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        evaluate the learned policy\n",
    "        \"\"\"\n",
    "        self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "        self.student.policy.eval()\n",
    "        mean_reward, std_reward = evaluate_policy(self.student, self.env, n_eval_episodes=10)\n",
    "        return mean_reward, std_reward\n",
    "            \n",
    "\n",
    "dagger_args = dict(\n",
    "    lr = 0.01,                # learning rate\n",
    "    bs = 2,                   # batch size\n",
    "    schedular_gamma = 0.7,    # learning rate scheduler\n",
    "    epochs = 4,               # epochs of training\n",
    "    dagger_rounds = 2,        # number of interactions with experts\n",
    "    num_expert_demo = 150,    # number of demonstrations needed from expert per interaction\n",
    "    num_expert_demo_dagger = {0: 100, 1: 200}\n",
    ")        \n",
    "\n",
    "\n",
    "seed_everything(30)\n",
    "student2 = A2C('MlpPolicy', env_name, verbose=1)\n",
    "dagger = DAgger(env, expert, student2, dagger_args)\n",
    "mean, std = dagger.evaluate()\n",
    "print('=' * 20)\n",
    "print(f'Before applying DAgger, the reward of student is {mean} +/- {std}')\n",
    "\n",
    "dagger.train()\n",
    "mean, std = dagger.evaluate()\n",
    "print('=' * 20)\n",
    "print(f'After applying DAgger, the reward of student is {mean} +/- {std}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f343",
   "metadata": {},
   "source": [
    "**Understanding the performance of DAgger.** \n",
    "\n",
    "<b style=\"color:red\">Question 4 (5 pts).</b> How many demonstrations are needed from experts? How many iterations are needed? Compare DAgger wit BC, are there any improvements? Can you explain the reasons behind the improvement?\n",
    "\n",
    "\n",
    "<b style=\"color:red\">Question 5 (5 pts).</b> Fix `dagger_rounds=2`. Let [a, b] denote using $a$ and $b$ demonstrations in the first and second interaction with the expert, respectively. Modify the code above to test the reward with [50, 250], [100, 200], [200, 100], and [250, 50]. What do you find?\n",
    "\n",
    "\n",
    "\n",
    "**Extension reading.** While DAgger is simple, its idea is useful to RLHF; see a recent [paper](https://arxiv.org/pdf/2404.08495.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3887f34",
   "metadata": {},
   "source": [
    "**Question 4**: We start with an initial dataset consisting of 150 demonstrations from the expert. For each epoch, we aggregate our dataset with 150 more demonstrations from the expert. Therefore, the total number of demonstrations is given by \n",
    "$$150 + \\text{epochs} \\times \\text{dagger rounds} \\times 150$$\n",
    "This equates to 1350 samples. In terms of the number of iterations, i.e, optimization steps required equates to $0.5 \\times (150+300+450+600+750+900+1050+1200+1350)$, which is 3375 optimization steps. In terms of performance, DAgger performs better, but also requires more iterations and demonstrations from the expert.\n",
    "\n",
    "**Question 5**: The total reward remains the same. The key here is that the total number of demonstrations from the expert and the number of optimization steps remains the same, regardless of the configuration. This would indicate that the performance is strongly dependent on the total number of samples and optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a6261",
   "metadata": {},
   "source": [
    "### Step 4. Policy aggregation <b style=\"color:red\">(15 points) </b>\n",
    "\n",
    "Next, we focus on another online IL algorithm, SMile. Different from DAgger, SMile aggregates policy rather than datasets. A step-by-step implementation can be found below.\n",
    "\n",
    "- Step 1. Run $\\pi_t$ to get dataset $\\hat{\\cal D}_t = \\{ s_1, s_2, .... \\}$. Let the expert label these states in $\\hat{\\cal D}_t$ and get ${\\cal D}_t = \\{ s_1, a_1, s_2, a_2 .... \\}$\n",
    "\n",
    "\n",
    "- Step 2. train $\\hat{\\pi}_{t+1}$ from data ${\\cal D}_t = \\{ s_1, a_1, s_2, a_2, ...\\}$\n",
    "\n",
    "- Step 3. Policy aggregation $\\pi_{t+1} = (1 - \\beta) \\pi_t + \\beta \\hat{\\pi}_t$, and go to step 1.\n",
    "\n",
    "\n",
    "<b style=\"color:red\">Question 4 (15 pts).</b> Implement and test the SMile algorithm by completing lines marked with **#TODO**. After the code is finished, it should get a final reward around 500. There is no need to tune hyperparameters. Analyze the data efficiency (how many demonstrations and how many optimization iterations), and compare with BC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3aa25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class AggregatedPolicies:\n",
    "    def __init__(self, beta=0.9):\n",
    "        \"\"\"\n",
    "        The aggregated policy. One way to achieve this is to store all the policies, \n",
    "        and sample them properly. (You should figure out what does it mean by 'properly'.)\n",
    "        \n",
    "        :param beta: the weights for different polices\n",
    "        \n",
    "        Example: Consider aggregating 3 policies, the polices and weights should be\n",
    "        self.policeis = [policy1, policy2, policy3]\n",
    "        self.weights = [(1 - beta)^2 * beta, (1 - beta) * beta, beta]\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.policies = []\n",
    "        self.weights = None\n",
    "    \n",
    "    def append(self, policy):\n",
    "        \"\"\"\n",
    "        Aggregate a new policy and update self.weight accordingly\n",
    "        \"\"\"\n",
    "        n = len(self.policies)\n",
    "        self.policies.append(policy)\n",
    "        \n",
    "        if self.weights is None:\n",
    "            self.weights = np.array([1.0])\n",
    "        else:\n",
    "            self.weights *= (1 - self.beta)\n",
    "            self.weights = np.append(self.weights, [self.beta])\n",
    "        \n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=False):\n",
    "        \"\"\"\n",
    "        Given observation, return an action from aggregated policy.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "    \n",
    "                \n",
    "\n",
    "class SMILe:\n",
    "    def __init__(self, env, expert, student, args):\n",
    "        self.env = env\n",
    "        self.student = student\n",
    "        self.expert = expert\n",
    "        self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 0\n",
    "        \n",
    "        # stores all the policies, and aggregated them when needed.\n",
    "        self.aggregated_policy = AggregatedPolicies(args['smile_beta'])\n",
    "        \n",
    "    \n",
    "    def expert_demonstration(self, num_data=200):\n",
    "        observations = np.empty((num_data,) + self.env.observation_space.shape)\n",
    "        actions = np.empty((num_data,) + self.env.action_space.shape)\n",
    "        obs, _ = self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "\n",
    "        for i in range(int(num_data)):\n",
    "            action, _ = self.expert.predict(obs, deterministic=True)\n",
    "            observations[i], actions[i] = obs, action\n",
    "            obs, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "            if done:\n",
    "                obs, _ = self.env.reset(seed=self.seed)\n",
    "                self.seed += 1\n",
    "            \n",
    "        return ExpertData(observations, actions)\n",
    "    \n",
    "    def student_transition_with_expert_demonstration(self, num_data=200):\n",
    "        observations = np.empty((num_data,) + self.env.observation_space.shape)\n",
    "        actions = np.empty((num_data,) + self.env.action_space.shape)\n",
    "        \n",
    "        obs, _ = self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "        \n",
    "        # TODOï¼š Gather expert demonstrations\n",
    "        # update: there is a loop, and the if-done code below is inside this loop\n",
    "        \n",
    "            if done:\n",
    "                obs, _ = self.env.reset(seed=self.seed)\n",
    "                self.seed += 1\n",
    "            \n",
    "        return ExpertData(observations, actions)\n",
    "        \n",
    "        \n",
    "    def bc_train(self, demonstration):    \n",
    "        \"\"\"\n",
    "        Train the student using behavioral cloning. The code here is modified from BehavioralCloning.train()\n",
    "        \n",
    "        :param demonstration: ExpertData that contains expert demonstration\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.student.policy.to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.args['lr'])\n",
    "        lr_scheduler = StepLR(optimizer, step_size=1, gamma=self.args['schedular_gamma'])\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(1, self.args['epochs'] + 1):\n",
    "            for batch_idx, (data, target) in enumerate(demonstration):\n",
    "                \n",
    "                # get expert demonstration\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # get student response\n",
    "                dist = model.get_distribution(data)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target = target.long()\n",
    "                \n",
    "                # update student\n",
    "                loss = criterion(action_prediction, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            lr_scheduler.step()\n",
    "                \n",
    "        self.student.policy = model\n",
    "    \n",
    "    \n",
    "    def evaluate(self, policy):\n",
    "        self.env.reset(seed=self.seed)\n",
    "        self.seed += 1\n",
    "        self.student.policy.eval()\n",
    "        mean_reward, std_reward = evaluate_policy(policy, self.env, n_eval_episodes=10)\n",
    "        return mean_reward, std_reward\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        for smile_round in range(self.args['smile_rounds']):\n",
    "            if smile_round == 0:\n",
    "                self.demo = self.expert_demonstration(num_data=self.args['num_expert_demo'])\n",
    "            else: \n",
    "                self.demo = self.student_transition_with_expert_demonstration(num_data=self.args['num_expert_demo'])\n",
    "            \n",
    "            # TODO: training\n",
    "            \n",
    "            \n",
    "smile_args = dict(\n",
    "    lr = 0.01,                # learning rate\n",
    "    bs = 2,                   # batch size\n",
    "    schedular_gamma = 0.7,    # learning rate scheduler\n",
    "    epochs = 5,               # training epochs\n",
    "    smile_rounds = 4,         # number of interactions with experts\n",
    "    num_expert_demo = 100,    # number of demonstrations needed from expert per interaction\n",
    "    smile_beta = 0.8,         # weight of policies in SMILe\n",
    ")\n",
    "\n",
    "seed_everything(30)\n",
    "student3 = A2C('MlpPolicy', env_name, verbose=0)\n",
    "smile = SMILe(env, expert, student3, smile_args)\n",
    "mean, std = smile.evaluate(smile.student)\n",
    "print('=' * 20)\n",
    "print(f'Before applying SMILe, the reward of student is {mean} +/- {std}')\n",
    "\n",
    "smile.train()\n",
    "\n",
    "mean, std = smile.evaluate(smile.aggregated_policy)\n",
    "print('=' * 20)\n",
    "print(f'After applying SMILe, the reward of student is {mean} +/- {std}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51d23f",
   "metadata": {},
   "source": [
    "### Step 4. Bonus <b style=\"color:red\">(10 points) </b>\n",
    "\n",
    "Implement A2C by yourself. Use this A2C expert to perform BC or DAgger on a PPO or A2C student.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332d485",
   "metadata": {},
   "source": [
    "# Exercise 2. Inverse RL\n",
    "\n",
    "In this exercise you work on max-margin inverse RL. For this we need to assume that the reward can be written as an inner as an inner product of some vector and the features of the state. A good example that fulfills this criterion is a simple gridworld environment.\n",
    "\n",
    "You can load the environment with all libraries that you will need for this exercise from the environment.yml (if you use conda) or from the requirements.txt (if you use pip). Note the exercise can be solved without additional libraries. If you feel like you absolutely need them make a note of this- (including install instructions/environment/requirements.txt etc.) so we can add them to our test environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87011600",
   "metadata": {},
   "source": [
    "## Set-up \n",
    "\n",
    "We begin by setting up a simple 5*5 gridworld environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GridWorldEnvironment\n",
    "import plot\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "#generate environment of type 2\n",
    "env = GridWorldEnvironment(prop = 0.0, env_type = 3)\n",
    "## plot the current environment\n",
    "plot.plot_reward(env.w, 5, title=\"\", tdw=False, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d38bc",
   "metadata": {},
   "source": [
    "See below some example code on how to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N states: \", env.n_states)\n",
    "start_state_id=env.get_starting_index()\n",
    "print(\"Starting state: \", start_state_id)\n",
    "start_state = env.get_random_initial_index()\n",
    "print(\"Starting state: \", start_state)\n",
    "poss_actions = env.get_possible_actions(start_state)\n",
    "print(\"Possible actions: \", poss_actions)\n",
    "print(\"The possible actions are represented as follows: 0 is up, 1 is right, 2 is down, 3 is left\")\n",
    "#take action up\n",
    "next_state_features, reward, terminal = env.step(0)\n",
    "print(\"Next state features: \", next_state_features)\n",
    "print(\"next state: \", env.get_current_state())\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Terminal: \", terminal)\n",
    "# take action left\n",
    "next_state_features, reward, terminal = env.step(3)\n",
    "print(\"Next state features: \", next_state_features)\n",
    "print(\"next state: \", env.get_current_state())\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Terminal: \", terminal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df343d5e",
   "metadata": {},
   "source": [
    "Now we want to implement value iteration as an RL algorithm to use to generate the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76aa297",
   "metadata": {},
   "source": [
    "## Exercise 2.1 (5pts) Train Expert Policy (Complete the code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_algorithm:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_space = env.get_n_states()\n",
    "        self.action_space = env.get_n_actions()\n",
    "        self.values = np.zeros(self.state_space)\n",
    "        self.q_values = np.zeros((self.state_space, self.action_space))\n",
    "        self.policy = np.zeros(self.state_space, dtype=int)\n",
    "\n",
    "    def value_iteration(self, max_iterations=100,tol=1e-9):\n",
    "        discount_factor = self.env.get_discount_factor()\n",
    "        #initialise new_values randomly between 0 and 1\n",
    "        new_values = np.random.rand(self.state_space)\n",
    "        for i in range(max_iterations):\n",
    "            old_values = self.values.copy()\n",
    "            for state in range(self.state_space):\n",
    "                q_values = []\n",
    "                for action in range(self.action_space):\n",
    "                    #TODO students should implement the Q-value computation, \n",
    "                    # Hinte use: state_step(self, state_index, a)\n",
    "                    \n",
    "                #TODO impplement the value update and policy update\n",
    "                \n",
    "            self.values = new_values\n",
    "            if np.max(np.abs(old_values - self.values)) < tol:\n",
    "                break\n",
    "                \n",
    "        #calculate the q_values from the values\n",
    "        self.q_values = np.zeros((self.state_space, self.action_space))\n",
    "        for state in range(self.state_space):\n",
    "            for action in range(self.action_space):\n",
    "                next_state, reward, done = self.env.state_step(state, action)\n",
    "                self.q_values[state, action] = reward + discount_factor * self.values[next_state] * (not done)\n",
    "        return self.values, self.q_values, self.policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecb43c",
   "metadata": {},
   "source": [
    "If your code was correct, we should get a meaningful plot below of the value and policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a418e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the value iteration algorithm\n",
    "rl = RL_algorithm(env)\n",
    "values, q_values, expert_policy = rl.value_iteration()\n",
    "#plot the values \n",
    "plot.plot_value_policy(values,expert_policy,5, title=\"Values\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc6ac1",
   "metadata": {},
   "source": [
    "Now that we have an optimal policy, we can try and recover the reward function from it using max-margin RL. For this we need to generate trajectories to calculate the feature expectation as follows :\n",
    "$$\\mu(\\pi)=E\\left[\\sum_{t=0}^{\\infty} \\gamma^t \\phi\\left(s_t\\right) \\mid \\pi\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f67362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate| expert trajectory, using the optimal policy policy until the end of the episode\n",
    "def generate_trajectory(env, policy, state = None, max_steps=100):\n",
    "    done = False\n",
    "    if state is None:\n",
    "        state = env.get_random_initial_index()\n",
    "    states = [state]\n",
    "    actions = []\n",
    "    step = 0\n",
    "    while not done and step < max_steps:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done = env.state_step(state, action)\n",
    "        state = next_state\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        step += 1\n",
    "    return states, actions\n",
    "states, actions = generate_trajectory(env, expert_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the feature expectations using the expert trajectory,\n",
    "# env.get_discount_factor and env.get_feature\n",
    "def generate_feature_expectations(env, states, discount_factor):\n",
    "    feature_expectations = np.zeros(env.get_features_dim())\n",
    "    for i, state_id in enumerate(states):\n",
    "        feature_expectations += discount_factor**i * env.get_features(state_id=state_id)\n",
    "    return feature_expectations\n",
    "\n",
    "def generate_MC_feature_expectations(env, policy, discount_factor, num_episodes=1000):\n",
    "    feature_expectations = np.zeros(env.get_features_dim())\n",
    "    for i in range(num_episodes):\n",
    "        env.reset()\n",
    "        states, actions = generate_trajectory(env, policy)\n",
    "        feature_expectations += generate_feature_expectations(env, states, discount_factor)/num_episodes\n",
    "    return feature_expectations\n",
    "\n",
    "expert_features = generate_MC_feature_expectations(env, expert_policy, env.get_discount_factor(), num_episodes=10000)\n",
    "print(\"Feature expectations: \\n\", expert_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b05a5",
   "metadata": {},
   "source": [
    "## Exercise 2.2 (10pts) implement max margin IRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b1f6c",
   "metadata": {},
   "source": [
    "In the following implement the max-margin IRL algorithm presented in the lecture (see also the following paper https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf for a detailed discussion and these slides from epfl for a concise explanation https://moodlearchive.epfl.ch/2020-2021/pluginfile.php/2744660/mod_resource/content/2/lecture%2012.pdf) For this you may use the following functions which solve the quadratic optimization problems:\n",
    "$$\\begin{array}{cl}\\max _{t, w} & t \\\\ \\text { s.t. } & w^T \\mu_E \\geq w^T \\mu^{(j)}+t, j=0, \\ldots, i-1 \\\\ & \\|w\\|_2 \\leq 1\\end{array}$$\n",
    "and\n",
    "\n",
    "$\\min \\left\\|\\mu_E-\\mu\\right\\|_2$ s.t. $\\mu=\\sum_i \\lambda_i \\mu^{(i)}, \\lambda_i \\geq 0, \\sum_i \\lambda_i=1 $\n",
    "\n",
    "You can reuse the value iteration above (the gridworld environment allows generating environments with \"preset\" w), or you can find the optimal policy in any way you like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_w_t(mu_j, mu_E, i):\n",
    "    \"\"\"\n",
    "    Optimize portfolio to maximize t.\n",
    "    \n",
    "    Parameters:\n",
    "    - mu_j: A list or numpy array of expected returns for each asset (mu_j).\n",
    "    - mu_E: The expected return of the portfolio (mu_E).\n",
    "    - i: The number of assets.\n",
    "    \n",
    "    Returns:\n",
    "    - The optimal value of t and the optimal weight vector w as a numpy array.\n",
    "    \"\"\"\n",
    "    # Ensure mu_j is a numpy array\n",
    "    mu_j = np.array(mu_j)\n",
    "\n",
    "    # Define the optimization variables\n",
    "    w = cp.Variable(len(mu_E))\n",
    "    t = cp.Variable()\n",
    "\n",
    "    # The objective is to maximize t\n",
    "    objective = cp.Maximize(t)\n",
    "\n",
    "    # Define the constraints\n",
    "    constraints = [w.T @ mu_E >= w.T @ mu_j[j] + t for j in range(i)]\n",
    "    constraints += [cp.norm(w, 2) <= 1]\n",
    "\n",
    "    # Set up and solve the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    # Extract the solution\n",
    "    optimal_t = t.value\n",
    "    optimal_w = w.value\n",
    "\n",
    "    return optimal_t, optimal_w\n",
    "\n",
    "\n",
    "def minimize_distance(target_mu, mus):\n",
    "    \"\"\"\n",
    "    Minimize the Euclidean distance between a target mu and a combination of given mus.\n",
    "    \n",
    "    Parameters:\n",
    "    - target_mu: The target expected return (mu_E).\n",
    "    - mus: A list or 2D numpy array where each column represents an asset's expected return (mu^(i)).\n",
    "    \n",
    "    Returns:\n",
    "    - The optimal weight vector lambda as a numpy array.\n",
    "    \"\"\"\n",
    "    # Ensure mus is a numpy array\n",
    "    mus = np.array(mus)\n",
    "    i = mus.shape[0]  # Number of mus\n",
    "    lambdas = cp.Variable(i)  # The weights we want to optimize\n",
    "\n",
    "    # Objective: Minimize the 2-norm of the difference\n",
    "    objective = cp.Minimize(cp.norm(target_mu - lambdas @ mus, 2))\n",
    "\n",
    "    # Constraints: lambda_i >= 0 for all i, and sum(lambda_i) == 1\n",
    "    constraints = [lambdas >= 0, cp.sum(lambdas) == 1]\n",
    "\n",
    "    # Set up and solve the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    return lambdas.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629b6d8",
   "metadata": {},
   "source": [
    "#### Exercise 2.2 (9 pts) Complete the code below:\n",
    "Here you need to implement the max margin IRL aglorithm. Note you need to return the generated policy in a one-hot encoded format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxMarginIRL:\n",
    "    def __init__(self, env, expert_features, epsilon=1e-4):\n",
    "        self.env = env\n",
    "        self.expert_features = expert_features\n",
    "        self.epsilon = epsilon\n",
    "        self.policies = [np.ones(shape=env.get_n_states(), dtype=int)]\n",
    "        self.feature_expectations = [self.compute_feature_expectations(self.policies[0])]\n",
    "\n",
    "    def compute_feature_expectations(self, policy):\n",
    "        # compute the feature expectations using the trajectory\n",
    "        feature_expectations = generate_MC_feature_expectations(env, policy, env.get_discount_factor(), num_episodes=600)\n",
    "        return feature_expectations\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the algorithm to compute the final policy.\n",
    "\n",
    "        This function iteratively generates feature expectations until a certain threshold is reached.\n",
    "        It optimizes the weights and computes the final policy based on the expert features and generated feature expectations.\n",
    "\n",
    "        Returns:\n",
    "        - w: The optimized weights. Shape (n_features,)\n",
    "        - t: The final threshold value. Shape (1,)\n",
    "        - final_policy: The computed final policy. Shape (n_states, n_actions)\n",
    "        - policies: The list of policies generated during the iterations. Shape (n_policies, n_states)\n",
    "        - feature_expectations: The list of feature expectations generated during the iterations. Shape (n_policies, n_features)\n",
    "        \"\"\"\n",
    "        # generate feature expectations until t is small enough\n",
    "        i = 1\n",
    "        while True and i < 100:\n",
    "             \n",
    "            print(\"current policy: \", self.policies[-1])\n",
    "            print(\"Iteration: \", i, \" t: \", t)\n",
    "            # TODO implement this using optmize_w_t\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        # Find the closest feature expectation in the convex hull using minimize distance\n",
    "        #TODO \n",
    "        lambdas = \n",
    "\n",
    "        # Compute the final policy pi=lambda1*pi1+...+lambda_k*pik\n",
    "        # note that we need to translate the policies of shape (25,) to (25,4) using one-hot encoding \n",
    "        final_policy = np.zeros((self.env.get_n_states(), self.env.get_n_actions()))\n",
    "        for i, policy in enumerate(self.policies):\n",
    "            final_policy += lambdas[i] * np.eye(self.env.get_n_actions())[policy.astype(int)]\n",
    "        #Compute the final feature expectations\n",
    "        final_feature_expectations = np.zeros(self.env.get_features_dim())\n",
    "        for i, feature_expectation in enumerate(self.feature_expectations):\n",
    "            final_feature_expectations += lambdas[i] * feature_expectation        \n",
    "\n",
    "        return w, t, final_policy, final_feature_expectations, self.policies, self.feature_expectations\n",
    "    \n",
    "max_margin_irl = MaxMarginIRL(env, expert_features)\n",
    "w, t, final_policy, final_features, policies, feature_expectations = max_margin_irl.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213962f6",
   "metadata": {},
   "source": [
    "Now we want to analyse the result. First we look at the difference of the feature expectation under the optimal policy and the we plot the policy and the corresponding reward function and compare to our initial initialisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3946924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance of feature expectation\n",
    "print(\"Distance of feature expectations: \", np.linalg.norm(final_features - expert_features))\n",
    "#turn the final policy into a deterministic policy\n",
    "final_policy_det = np.argmax(final_policy, axis=1)\n",
    "# Plot of policy and recovered rewards\n",
    "plot.plot_reward_policy(w, final_policy_det, 5, title=\"Final deterministic policy\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29596678",
   "metadata": {},
   "source": [
    "#### Exercise 2.3 (1pt) \n",
    "Note that the feature expecations and recovered policy match quite closely. However, the rewards vary significantly and it might hard to see the original cross. Explain this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a99a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edb08385",
   "metadata": {},
   "source": [
    "# Exercise 3. Maximum entropy IRL\n",
    "In this exercise you work on max entropy inverse RL. The simple gridworld environment of the previous exercise is expected to be used.\n",
    "\n",
    "<!-- ## For starters: some notation and theory -->\n",
    "\n",
    "<!-- **The usual MDP**:\n",
    "\n",
    "- states\n",
    "    $S = {s_i}$\n",
    "- actions $A = {a_i}$\n",
    "- transition dynamics $T = p(s_{i+1}|s_t, a_t)$\n",
    "- reward $R : S \\rightarrow \\mathbb{R}$\n",
    "\n",
    "**Trajectories**:\n",
    "- trajectory $\\tau = \\{ (s_0, a_0), (s_1, a_1) \\dots (s_T, a_T)\\}$\n",
    "- demonstration $\\mathcal{D} = \\{ \\tau_0, \\tau_1, \\dots \\tau_{D} \\}$\n",
    "\n",
    "**Features**:\n",
    "- $\\Phi : S \\rightarrow \\mathbb{R}^d$ with $\\Phi(\\tau) = \\Sigma_{s_i \\in \\tau}(s_i)$\n",
    "\n",
    "**Policies**:\n",
    "- stochastic policy $\\pi(a_i | s_i)$\n",
    "- learner policy $\\pi^{L}$\n",
    "- expert policy $\\pi^{E}$, we assume that the expert policy is close-to-optimal\n",
    "    $\\pi^{E} \\sim \\pi^{\\star}$\n",
    "- learned reward $r_L(s, a)$, suppose also that such reward is parametrized by $\\theta$\n",
    "- learned reward trajectory-wise $R_L(\\tau) = \\Sigma_{t=0}^{T} r_{L}(s_t, a_t)$ -->\n",
    "\n",
    "**Problem**\n",
    "\n",
    "*Given*:\n",
    "- state and action space\n",
    "- roll-outs trajectories from $\\pi^{E}$\n",
    "- possibly the transition model\n",
    "\n",
    "*Goal*:\n",
    "- recover the reward function $R$\n",
    "- then from the reward $R$, get the policy $\\pi$\n",
    "\n",
    "The agent is assumed to be attempting\n",
    "to optimize some function that linearly maps the features\n",
    "of each state, $f_{s_j} \\in \\mathbb{R}_f$,\n",
    "to a state reward value representing the agent reward for visiting that state.\n",
    "This function is parameterized by some reward weights, Î¸.\n",
    "\n",
    "**Challenges**\n",
    "\n",
    "- As seen in the previous exercise, the reward function which we recover is pretty different with respect to the one defined in the environment, this because the problem is undetermined\n",
    "- It is difficult to evaluate a learned reward, it may mismatch the original one\n",
    "- The demonstrations themselves may be sub-optimal\n",
    "\n",
    "**Preliminary results**\n",
    "\n",
    "The learner policy should visit, *in expectation*, the same features as the expert one.\n",
    "\n",
    "*Feature Expectation Matching*\n",
    "\n",
    "Reference [Abbel and Ng 2004](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)\n",
    "This matching is both necessary and sufficient to achieve the same performance as the agent if the agent were in fact solving an MDP\n",
    "with a reward function linear in those features.\n",
    "\n",
    "$\\mathbb{E}_{\\pi^{E}} [\\Phi(\\tau)] = \\mathbb{E}_{\\pi^{L}} [\\Phi(\\tau)]$\n",
    "\n",
    "We want to find the reward function $R : S -> \\mathcal{R}$\n",
    "defining\n",
    "\n",
    "*Max entropy formulation*\n",
    "\n",
    "$$p(\\tau) = \\dfrac{1}{Z} \\exp{exp}(R_{L}(\\tau))$$\n",
    "where $R_{L}$ is the learned reward.\n",
    "And the normalization is $Z = \\int \\text{exp}(R_{L}(\\tau)) \\text{d}\\tau$.\n",
    "Now\n",
    "$$\\text{max}_{R_{L}} \\underbrace{\\Sigma_{\\tau \\in \\mathcal{D}} \\text{log} p_{R_{L}} (\\tau)}_{\\mathcal{L}}$$\n",
    "\n",
    "## Question 3.1 (1pt)\n",
    "Compute the gradient update $\\nabla_\\theta R_L(\\theta)$ of the learned reward $R_L(\\theta)$,  which to maximize the entropy with respect to the collected demonstrations $\\mathcal{D}$.\n",
    "\n",
    "**Fill the proof here in latex format**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467bc56",
   "metadata": {},
   "source": [
    "## Algorithm: Maximum entropy Inverse RL\n",
    "From [Ziebart 2008](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf)\n",
    "\n",
    "*Setup*:\n",
    "- Gather expert demostrations $\\mathcal{D}$\n",
    "- Initialize $\\theta$ to some good guess\n",
    "\n",
    "*Iteration*:\n",
    "1. Solve for optimal policy $\\pi_{L}(a|s)$ with reward $r_{L}(\\theta)$\n",
    "2. Solve for the transition dynamics $p(s|r_L(\\theta))$\n",
    "3. Compute the gradient $\\nabla_{\\theta} \\mathcal{L}= -\\dfrac{1}{|\\mathcal{D}|}\\Sigma_{\\tau \\in \\mathcal{D}} \\dfrac{\\text{d}R_{L}(\\tau)}{\\text{d}\\theta} - {\\Sigma_s p(s | r_{L}(\\theta)) \\frac{\\text{d}r_{L}(s)}{\\text{d}\\theta}}$\n",
    "4. Update $\\theta$ with one gradient step\n",
    "5. Iterate to convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c83b6d",
   "metadata": {},
   "source": [
    "## Question 3.2 (1pt)\n",
    "We use the same environment and expert policy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b959ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reward\n",
    "plot.plot_reward(env.w, env.size, title=\"\", tdw=False, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the values \n",
    "plot.plot_value_policy(values, expert_policy, env.size, title=\"Values\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff830e",
   "metadata": {},
   "source": [
    "## Question 3.3 Generate Expert Demostrations (2pt)\n",
    "Generate 5 **grid-states** trajectories of length at most 100 from the expert policy learned in the previous point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the expert trajectory, using the optimal policy policy until the end of the episode\n",
    "def generate_random_demostrations(env, policy, n_trajectories, max_steps = 1000):\n",
    "    states_trajectories = []\n",
    "    actions_trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        # randomize the initial state convert to grid states\n",
    "        # TODO\n",
    "        \n",
    "        states_trajectories.append(TODO)\n",
    "        actions_trajectories.append(TODO)\n",
    "    return states_trajectories, actions_trajectories\n",
    "\n",
    "states_trajectories, actions_trajectories = generate_random_demostrations(env, expert_policy, 100)\n",
    "\n",
    "# Print the demonstrations\n",
    "print(states_trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbe468",
   "metadata": {},
   "source": [
    "## Question 3. Implement the algorithm (10pt)\n",
    "Implement the missing functions in this class to calculate the gradient as in the referenced paper [Ziebart 2008](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf).\n",
    "Complete accordingly: `compute_feature_expectations`, `compute_intial_probabilities`, `compute_reward`, `backward_pass` and `forward_pass`\n",
    "These functions will be called to compute the gradient in `compute_gradient` (which is provided, together the remaining functions). Be careful as you need to provide the correct outputs for the algorithm to correctly work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEntropyIRL():\n",
    "    def __init__(self, demonstrations, optimizer, environment, max_iter):\n",
    "        self.states_trajectories = demonstrations\n",
    "        self.optimizer = optimizer\n",
    "        self.env = environment\n",
    "        self.max_iter = max_iter\n",
    "        self.terminal = self.env.terminal_indexes[0]\n",
    "        self.n_features = self.env.features_dim\n",
    "        self.n_actions = self.env.n_actions\n",
    "        self.n_states = self.env.n_states\n",
    "        self.p_transition = self.env.P\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        return self.env.features\n",
    "    \n",
    "    def compute_feature_expectations(self):\n",
    "        # TODO Process from demonstrations trajectories, get the feature expectations\n",
    "        \n",
    "        NotImplemented\n",
    "        \n",
    "        return e_features\n",
    "\n",
    "    def compute_initial_probabilities(self):\n",
    "        # TODO Implement\n",
    "        for t in self.states_trajectories:\n",
    "            \n",
    "            NotImplemented\n",
    "        \n",
    "        return p_initial\n",
    "\n",
    "    def compute_reward(self, theta):\n",
    "        # TODO Compute state reward, as a scalar\n",
    "        return NotImpelemented\n",
    "    \n",
    "    def backward_pass(self, reward, eps_esvf = 1e-3):\n",
    "        # TODO complete the backward pass as described in the reference\n",
    "\n",
    "        return za, zs\n",
    "\n",
    "    def compute_local_action_probabilities(self, za, zs):\n",
    "        # Compute local action probabilities\n",
    "        p_action = za / zs[None, :]\n",
    "        return p_action\n",
    "        \n",
    "    def forward_pass(self, p_action, eps_esvf = 1e-3):\n",
    "        # TODO\n",
    "        # compute the forward-pass\n",
    "        # Hints set-up transition matrices for each action\n",
    "        p_transition = NotImplemented # TODO\n",
    "    \n",
    "        # actual forward-computation of state expectations\n",
    "        d = np.zeros(self.n_states)\n",
    "        delta = np.inf\n",
    "        while delta > eps_esvf:\n",
    "            # TODO loop until the convergence criterion is met\n",
    "            NotImplemented\n",
    "            delta, d = np.max(np.abs(d_ - d)), d_\n",
    "        return d\n",
    "    \n",
    "    def gradient_computation(self, theta):\n",
    "        # This function recovers the gradient to use in optimization\n",
    "        reward = self.compute_reward(theta)\n",
    "        za, zs = self.backward_pass(reward)\n",
    "        p_action = self.compute_local_action_probabilities(za, zs)\n",
    "        e_svf = self.forward_pass(p_action)\n",
    "        gradient = self.e_features - self.features.T.dot(e_svf)\n",
    "        return gradient\n",
    "    \n",
    "    def run(self, initial_guess, eps = 1e-4):\n",
    "        # Gradient ascent optimization\n",
    "        self.theta = theta = initial_guess(self.n_features)\n",
    "        self.optimizer.init(theta)\n",
    "\n",
    "        # Compute static properties from demonstrations\n",
    "        self.e_features = self.compute_feature_expectations()\n",
    "        self.p_initial = self.compute_initial_probabilities()\n",
    "\n",
    "        delta = np.inf\n",
    "        counter = 0\n",
    "        \n",
    "        while delta > eps and counter < self.max_iter:\n",
    "            theta_old = theta.copy()\n",
    "            gradient = self.gradient_computation(theta)\n",
    "\n",
    "            self.optimizer.step(gradient)\n",
    "            delta = np.max(np.abs(theta_old - self.theta))\n",
    "            if counter % 100 == 0 or delta < eps:\n",
    "                print(\"Iteration : \", counter, \"Tolerance : \", delta)\n",
    "            counter += 1\n",
    "\n",
    "        return self.compute_reward(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c37395",
   "metadata": {},
   "source": [
    "## Question 3.3 Run the Max Entropy IRL (1pt)\n",
    "Run the algorithm on the state trajectories generated as demostrations and plot and comment the result.\n",
    "Select a proper initial guess for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization import ExponentiatedSGA, Constant, linear_decay\n",
    "optimizer = ExponentiatedSGA(lr = linear_decay(lr0 = 0.2))\n",
    "max_iter = 1e3\n",
    "tolerance = 1e-5\n",
    "# TODO select a good initial guess (picking an initial_state) for the optimization loop\n",
    "initial_state = None # TODO\n",
    "initial_guess = Constant(initial_state)\n",
    "solver = MaxEntropyIRL(states_trajectories, optimizer, env, max_iter)\n",
    "reward = solver.run(initial_guess, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot.plot_reward_policy(reward, expert_policy, env.size, title=\"Final deterministic policy\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46718b95",
   "metadata": {},
   "source": [
    "Compare this with the plot of the max margin IRL. What do you notice about the recovered rewards?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
